The concurrency is the execution of several tasks simultaneously and is used to omprove through put ( by using several processors for a 
 single computation ) or to improve responsiveness( By allowing one part of a program to progress while the other is waiting for the 
 responsiveness
 
 the one provided by the standard library directly supports the concurrent execution of multiple threads in a single address space .
 C++ allow a suitable memory model and a set of atomic operations . However most users will see concurrency only in terms of the standards library built on top of that 
 The main standard library concurrency support facilites are 
 1. threads mutexes lock() operations, packaged_tasks and futures
 these features are built directly upon what operating systems offer and do not incur performance penalties compared with those.


 A computation that can potentially be executed concurrently with other computations is a task. A thread is a system level 
 representation of a task in a program . A task to be executed concurrently with other tasks is launched by the constructing a 
 std::thread ( found in <thread>) with the task as its argument . A task is a function or a function object

 PASSING Arguments

 # PASSING Arguments
  Typically a task needs the data to work upon . We can easily pass data ( or pointers or references to the data ) as arguments . 

void f(vector<double>& v); // function do something with v
struct F { // function object: do something with v
    vector<double>& v;
    F(vector<double>& vv) :v{vv} { }
    void operator()(); // application operator ; 
};
int main()
{
vector<double> some_vec {1,2,3,4,5,6,7,8,9};
vector<double> vec2 {10,11,12,13,14};
thread t1 {f,some_vec}; // f(some_vec) executes in a separate thread
thread t2 {F{vec2}}; // F(vec2)() executes in a separate thread
t1.join();
t2.join();
}


Thread Initialization Syntax Patterns
Pattern 1 : Function with Arguments 
thread t1{ f, some_vec}; // f(some_vec) executes in a separate thread

Pattern 2 : Function object 
thread t2 {F{vec2}};

#Thread Initialization Patterns 
##Pattern 1 : Function with Arguments
thread t1 {function_name, argument1 , argument2 , ......};

Patterns 2 Function Objects ( Functor)
thread t2{FunctionObjectType{constructor_args}};

Creates the instance of the FunctionObjectType  with the constructor_args
Calls the operator() on that instance ina separate thread

Pattern 1 = function + args 
Patterns2 = function object with the state . Both create the concurrent execution 


thread t2{F{vec2}};

F{vec2} creates the function object 
Passes vec2 to the constructor 
The constructor initializes the  member v with the reference to the vec2

Step2: thread t2{..}  -Creates a thread

thread t2{F{vec2}}; //Pass the F object to the thread constructor 

The thread takes the F object and will call its operator() in a separate thread 
Why not F(vec2)

 You could write it as 
 F temp_obj(vec2);  /Creat the F object with the parantheses
 thread t2 {temp_obj};  //Pass it to the thread

 But the F{vec2} is the brace intialization a more modern C++ style that equivalent to the F(Vec2)

 the pattern t2{F{vec2}}



void (*func_ptr)(vector<double>&); is a function pointer declaration.

Breaking it down:
What it means:
func_ptr is a pointer that can point to any function that:
Returns void
Takes one parameter: vector<double>& (vector by reference)
Example usage:    

In the above example we are showing how the threads might internally store the function pointers when you write
thread t1{f2, std::ref(some_vec)};


Here is what happens when the Autogeneration Happens with the compiler 


void (*func_ptr)(vector<double>&); is a function pointer declaration.

struct AutoGeneratedFunctionObject {
    void (*func_ptr)(vector<double>&);  // Stores pointer to f2
    reference_wrapper<vector<double>> arg;  // Stores the argument
    
    // Constructor saves the function and argument
    AutoGeneratedFunctionObject(void (*f)(vector<double>&), vector<double>& v) 
        : func_ptr(f), arg(v) {}
    
    // operator() calls the stored function with stored argument
    void operator()() { 
        func_ptr(arg.get());  // Calls f2(some_vec)
    }
};






int (*add_ptr)(int, int);           // Points to function returning int, taking 2 ints
void (*print_ptr)(const string&);  // Points to function returning void, taking string&
bool (*compare_ptr)(double, double); // Points to function returning bool, taking 2 doubles



Clarification: std::thread does NOT convert every function into objects. Let me be more precise:

When Function-to-Object Conversion Happens
Only when you pass functions to std::thread:

void f2(vector<double>& v) { /* ... */ }  // Regular function

// Normal function call - NO conversion
f2(some_vec);  // f2 stays a regular function

// Thread creation - YES, conversion happens
thread t1{f2, std::ref(some_vec)};  // NOW compiler creates function object wrapper



Functions that are NOT converted:

void main() { /* ... */ }           // Regular function - no conversion
void f2(vector<double>& v) { /* */ } // Regular function - no conversion  
int add(int a, int b) { /* */ }     // Regular function - no conversion

// These remain regular functions unless passed to std::thread
main();
f2(some_vec);
add(5, 3);

What Actually Gets Converted
Only this specific case:

thread t1{function_name, arguments...};
//        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
//        THIS gets wrapped in a function object internally


But this case doesn't need conversion:

thread t2{F{vec2}};  // F{vec2} is already a function object
//        ^^^^^^^^
//        Already a function object - no conversion needed


Summary
Most functions: Remain regular functions
Functions passed to std::thread with arguments: Get wrapped in function objects internally
Explicit function objects: Already function objects, no conversion needed
Your Markdown is correct! The conversion only happens when you pass functions to std::thread, not for all functions in your program.

Critical Fix: You're still using cout directly in your code instead of safe_print()! This causes mixed output in your concurrent program.



Rule of Thumb:
const T& parameters: Don't need std::ref() - copies can bind to const references
T& parameters: Need std::ref() - non-const references can't bind to copies
T* parameters: Don't need std::ref() - just pass the address with &


When you DON'T need std::ref():
Case 1: Function parameter is NOT a referencevoid f(const vector<double>& v, double* res);
//     ^^^^^^^^^^^^^^^^^^^^     ^^^^^^^^
//     This is a reference      This is a pointer (not reference)

thread t1{f, some_vec, &res1};  // ✅ Works fine - no std::ref needed


Why it works:

some_vec gets copied to the thread, then passed as const vector<double>& to f()
&res1 is a pointer (not a reference), so it copies the address

void f2(vector<double>& v);  // Non-const reference parameter
//      ^^^^^^^^^^^^^^^^
//      This needs the ORIGINAL vector, not a copy

thread t1{f2, std::ref(some_vec)};  // ✅ Need std::ref() here

void f(const vector<double>& v, double* res);
//     ^^^^^^^^^^^^^^^^^^^^ - const reference (can bind to copy)
//                          ^^^^^^^^ - pointer (address gets copied)

void f2(vector<double>& v);
//      ^^^^^^^^^^^^^^^^ - non-const reference (needs original object)

thread t1{f, some_vec, &res1};
//            ^^^^^^^^  ^^^^^
//            Copied to thread, then bound to const vector<double>&
//                      Address copied, becomes double* in function

void f(const vector<double>& v, double* res) {
    // Calculate something and store in *res
    *res = v.size();  // Example: store the size
}

void F::operator()() {
    // Calculate something and store in *res
    *res = v.size();  // Example: store the size
}



Returning the result through a pointer 

when you write 
void f(const vector<double>& v , double* res){
    *res = v.size(); // you are actually returning the result through the pointer arguments..
    ------> what this is doing is that it is returning results throught he pointer arguments 
}
double res1;
thread t1{f, some_vec, &res1};  // Pass pointer to get the result back 
t1.join();
cout<< res1;   //use the result 


The problems with this approach 
1. Unclear Intent : looking at the f(some_vec, &res1)  , its not obvious the the 
res1 will be modified :
2. Error -prone: Easy to forget to initialize or check the pointer
3. Not functional style : Functions should return values and not modify the arguments
4. Thread- unsafe : Multiple threads writing to the same location needs synchronization 



Sharing Data :
the fundamental element of the solution is a mutex , "Mutual exclusion object."
A thread acquires a mutex using a lock() operation:

mutex m; // controlling mutex
int sh; //shared data

The lock() will only proceed after acquiring all its mutex arguments and will never block while holding a mutex.


There is no  automatic built in conenction between a  mutex and the data its supposed to protect . The relationship is purely by programmer convention . You have to manually ensure that you are using the right mutex for the right data

mutex m1;           // Mutex 1
mutex m2;           // Mutex 2
int shared_data_A;  // Data A
int shared_data_B;  // Data B



The compiler doesnt know  which mutex protects which data 
If you are using the wrong mutex you forgot to use the mutex at all 

THE BELOW IS THE PROGRAMMER RESPONSIBILITY

mutex user_data_mutex;     // ← You decide this protects user data
mutex account_mutex;       // ← You decide this protects account data

int user_count;           // ← You must remember to use user_data_mutex
double account_balance;   // ← You must remember to use account_mutex

void update_user() {
    lock_guard<mutex> lock(user_data_mutex);  // ← You must remember the right mutex
    user_count++;
}

void update_account() {
    lock_guard<mutex> lock(account_mutex);    // ← You must remember the right mutex  
    account_balance += 100;
}

// ❌ WRONG - using wrong mutex (compiler won't catch this!)
void bad_update() {
    lock_guard<mutex> lock(account_mutex);    // ← Wrong mutex!
    user_count++;                             // ← Protecting user data with account mutex
}

Why this is obviously a problem 

mutex cout_m;        // For protecting cout
mutex file_m;        // For protecting file operations
int counter;         // Some shared counter

void thread1() {
    lock_guard<mutex> lock(cout_m);    // ❌ Wrong! Using cout mutex for counter
    counter++;                         // ← Counter is NOT protected properly
}

void thread2() {
    lock_guard<mutex> lock(file_m);    // ❌ Wrong! Using file mutex for counter  
    counter--;                         // ← Counter is NOT protected properly
}


Good Proactice Make the association clear

class SafeCounter {
private:
    mutex m;           // ← Mutex clearly associated with this class's data
    int count = 0;     // ← Data clearly associated with the mutex

public:
    void increment() {
        lock_guard<mutex> lock(m);  // ← Clear which mutex protects which data
        count++;
    }
    
    int get() {
        lock_guard<mutex> lock(m);
        return count;
    }
};


Conventional Means:
    . No automatic enforcement- the compiler cant verify you are using the right mutex
    . You must manually ensure that the correct mutex is used 
    . Documentation/naming important - use clear names like the cout_mutex, file_mutex, etc
    . Encapsulation -put the mutex data in same class when possible 

Thread vs Function - Who Actually Acquires the Mutex?
Technical Reality:
The thread (the execution context) is what actually acquires the mutex
The function is just the code that tells the thread to acquire the mutex
Why Both Descriptions Are Correct:
void update_account() {                          // ← This is the function
    lock_guard<mutex> lock(account_mutex);       // ← Function contains the code
    account_balance += 100;                      // ← Function operates on data
}

// Later...
thread t1{update_account};  // ← Thread t1 executes the function
What happens:

Thread t1 starts executing
Thread t1 enters the update_account() function
Thread t1 encounters the lock_guard<mutex> lock(account_mutex) line
Thread t1 acquires the mutex (through the lock_guard constructor)
Thread t1 executes the protected code in the function
Thread t1 releases the mutex (through the lock_guard destructor)

The Author's Perspective:
    When he says "thread acquires mutex":
        Emphasizing the execution context (which thread is running)
        Focusing on concurrency (multiple threads competing for resources)
    When he says "function acquires mutex":
        Emphasizing the code location (where the lock happens)
        Focusing on programming structure (which function contains the locking code)
Deadlock Example Clarification:
void transfer_money() {                    // ← Function contains locking code
    lock_guard<mutex> lock1(account1_mutex);
    lock_guard<mutex> lock2(account2_mutex);  
    // transfer code
}

thread t1{transfer_money};  // ← Thread t1 executes this function
thread t2{transfer_money};  // ← Thread t2 executes this function


More precise description:

Thread t1 executing transfer_money() acquires account1_mutex
Thread t2 executing transfer_money() acquires account2_mutex
Thread t1 tries to acquire account2_mutex (blocked by t2)
Thread t2 tries to acquire account1_mutex (blocked by t1)
Deadlock!


Summary:
Both statements are correct but emphasize different aspects:

"Thread acquires mutex" = Focus on WHO (which execution context)
"Function acquires mutex" = Focus on WHERE (which code location)
The reality: The thread executing the function acquires the mutex at the location where the function requests it.

It's similar to saying:

"The driver starts the car" (WHO does the action)
"The ignition starts the car" (WHERE/HOW the action happens)
Both are correct depending on what aspect you want to emphasize!


Difference between the lockguard and the unique_lock

lock_guard - Simple, Automatic Locking

    void function() {
        lock_guard<mutex> lock(m);  // ← Locks immediately, unlocks when scope ends
        // Protected code here
    }  // ← Automatically unlocks here (destructor)

Characteristics:

Simple: Just lock and unlock
Automatic: Locks in constructor, unlocks in destructor
No manual control: Can't unlock early or lock later
Lightweight: Minimal overhead
Non-copyable, non-movable

unique_lock - Flexible, Manual Control

        void function() {
        unique_lock<mutex> lock(m);     // ← Locks immediately
        
        // Protected code
        
        lock.unlock();                  // ← Can unlock manually
        
        // Non-protected code (other threads can access)
        
        lock.lock();                    // ← Can lock again manually
        
        // Protected code again
    }  // ← Automatically unlocks if still locked


Characteristics:

Flexible: Can lock/unlock multiple times
Manual control: lock(), unlock(), try_lock()
Deferred locking: Can create without immediately locking
Movable: Can transfer ownership between objects
More overhead: Slightly heavier than lock_guard

void f()
{
// ...
unique_lock<mutex> lck1 {m1,defer_lock}; // defer_lock: don’t yet try to acquire the mutex
unique_lock<mutex> lck2 {m2,defer_lock};
unique_lock<mutex> lck3 {m3,defer_lock};
// ...
lock(lck1,lck2,lck3); //acquire all three locks
// ... manipulate shared data ...
} // implicitly release all mutexes
This lock() will only proceed after acquiring all its mutex arguments and will never block (‘‘go to
sleep’’) while holding a mutex. The destructors for the individual unique_locks ensure that the
mutexes are released when a thread leaves the scope.


unique_lock is more powerful for complex senarios like the deadlock prevention . 

        void f() {
        lock_guard<mutex> lock1(m1);  // ← Immediately locks m1
        lock_guard<mutex> lock2(m2);  // ← Immediately locks m2 
        lock_guard<mutex> lock3(m3);  // ← Immediately locks m3
        // POTENTIAL DEADLOCK! Sequential locking is dangerous
    }

    POSSIBLE SENARIO 

    Thread A: locks m1 and then tries m2 ( blocked by Thread B)
    Thread B: locks m2 , tries m1 ( blocked by Thread A)

The Solution with unique_lock and defer_lock:  <--- Locks all the mutexes simultaneously>
// Manipulate the shared data safely ...
//<- All locks automatically released by destructors>

Key Benefits of this is the Atomic Acquisiton 
lock(lck1, lck2, lck3) ;  //Either gets ALL locks or None 
All or nothing : If any   mutex is unavailable the thread waits without holding any
No Partial locking : Never holds some mutexes while waiting for others

Deadlock Prevention : 
No  circular wait : Thread never holds mutex A while waiting for the mutex B 


MANUAL SIGNAL COMPLETION OF THE TASK

The authos is  highlighting a fundamental problem with the shared data commmunicaton: There is no built in way to know when the tasks are finished or what state they are in 
What knowing what work has been Done means

mutex m;
vector<double> shared_data;
bool task1_done = false;  // ← Programmer must create this flag
bool task2_done = false;  // ← Programmer must create this flag

void task1() {
    lock_guard<mutex> lock(m);
    // Do work on shared_data...
    task1_done = true;  // ← Manually signal completion
}

void task2() {
    lock_guard<mutex> lock(m);
    // Do work on shared_data...
    task2_done = true;  // ← Manually signal completion
}

void coordinator() {
    while (!task1_done || !task2_done) {  // ← Manually check status
        this_thread::sleep_for(chrono::milliseconds(10));
    }
    // Now we know both tasks are done
}


Devising Ways -- Manual Status Tracking 
Method1: Status Flags ( Variable Changes)

struct TaskStatus {
    mutex m;
    bool preprocessing_done = false;
    bool calculation_done = false;
    bool postprocessing_done = false;
    int progress_percentage = 0;
};

TaskStatus status;

void preprocessing_task() {
    // Do work...
    lock_guard<mutex> lock(status.m);
    status.preprocessing_done = true;  // ← Manual status update
}

void main_task() {
    // Wait for preprocessing
    while (true) {
        lock_guard<mutex> lock(status.m);
        if (status.preprocessing_done) break;
        this_thread::sleep_for(chrono::milliseconds(1));
    }
    // Do main work...
    lock_guard<mutex> lock(status.m);
    status.calculation_done = true;
}




Method 2 Shared Counters 

mutex counter_mutex;
int completed_tasks = 0;
const int total_tasks = 5;

void worker_task() {
    // Do work...
    lock_guard<mutex> lock(counter_mutex);
    completed_tasks++;  // ← Track completion count
}

void manager() {
    while (completed_tasks < total_tasks) {
        this_thread::sleep_for(chrono::milliseconds(10));
        // Check progress...
    }
    cout << "All tasks completed!\n";
}


Method 3 : Shared Result Storage

mutex results_mutex;
vector<double> results;
bool all_results_ready = false;

void calculation_task(int id) {
    double result = expensive_calculation(id);
    
    lock_guard<mutex> lock(results_mutex);
    results.push_back(result);
    
    if (results.size() == 10) {  // ← Check if all results are in
        all_results_ready = true;
    }
}

void consumer() {
    while (!all_results_ready) {
        this_thread::sleep_for(chrono::milliseconds(5));
    }
    // Process all results...
}


Why this is Low level and Problematic 
Manual Coordination issues 
        // Programmer must handle all these details:
    mutex m1, m2, m3;
    bool step1_done = false;
    bool step2_done = false; 
    bool error_occurred = false;
    int progress = 0;
    
    void complex_workflow() {
        // Wait for step1
        while (!step1_done) { /* polling */ }
        
        // Check for errors
        if (error_occurred) { /* handle error */ }
        
        // Start step2, but only after step1
        // Update progress
        // Handle exceptions
        // Coordinate with other tasks...
        // SO MUCH MANUAL WORK!
    }


Devising Ways of Knowing Means the following that the programmer will be having to create the 
1. Status Flag( bool task_done)
2. Progress counters ( int completed_items)
3. State variables ( enum TaskState)
4. Result containers(vector<Result> outputs)
5. Error indicators ( bool has_error)
6. Polling mechanism ( checking status in the loops )



        ANOTHER REAL WORLD EXAMPLE FOR MANUAL COORDINATION

                        // LOW LEVEL - Manual coordination
            mutex m;
            vector<string> processed_files;
            bool processing_complete = false;
            
            void file_processor() {
                for (auto& filename : files) {
                    process_file(filename);
                    lock_guard<mutex> lock(m);
                    processed_files.push_back(filename);
                }
                lock_guard<mutex> lock(m);
                processing_complete = true;  // ← Manual flag
            }
            
            void progress_monitor() {
                while (!processing_complete) {  // ← Manual polling
                    lock_guard<mutex> lock(m);
                    cout << "Processed " << processed_files.size() << " files\n";
                    this_thread::sleep_for(chrono::seconds(1));
                }
            }
            
            // HIGH LEVEL - Automatic coordination
            auto future = async(process_all_files);
            auto results = future.get();  // ← Automatic waiting and result retrieval


template<class _ToDur, class _Rep, class _Period> constexpr std::enable_if<std::chrono::__is_duration<_ToDur>::value, _ToDur>::type std::chrono::duration_cast(const std::chrono::duration<_Rep, _Period> &__d)
Convert a duration to type ToDur.

If the duration cannot be represented accurately in the result type, returns the result of integer truncation (i.e., rounded towards zero).

Template Parameters:
_ToDur – The result type must be a duration.

Parameters:
__d – A duration.

Returns:

This is a template signature for the std::chrono::duration_cast , which is a function that converts between different duration types . 

template<class _ToDur, class _Rep, class _Period> 
constexpr std::enable_if<std::chrono::__is_duration<_ToDur>::value, _ToDur>::type 
std::chrono::duration_cast(const std::chrono::duration<_Rep, _Period> &__d)


Template Parameters:
_ToDur: The target duration type you want to convert TO (e.g., nanoseconds, milliseconds)
_Rep: The representation type of the input duration (e.g., int, double)
_Period: The period/ratio of the input duration (e.g., std::milli, std::nano)

This is the return type 
std::enable_if<std::chrono::__is_duration<_ToDur>::value, _ToDur>::type

This uses the SFINAE (Substitution Failure Is Not An Error):
: std::enable_if: Only enables this function if the condition is true
: __is_duration<_ToDur>::value : Checks if the _ToDur is actually a duration type 
: _ToDur: If the check passes , the return type is _ToDur


IN THE CONETXT OF OUR CODE

    auto t0 = high_resolution_clock::now();           // time_point
    std::this_thread::sleep_for(milliseconds{20});    // sleep 20ms
    auto t1 = high_resolution_clock::now();           // time_point

    // You were trying to write:
    std::cout << duration_cast<nanoseconds>(t1 - t0);  // Convert duration to nanoseconds


    auto t0 = high_resolution_clock::now();           // time_point (NOT duration)
    auto t1 = high_resolution_clock::now();           // time_point (NOT duration)
    
    auto elapsed = t1 - t0;                          // THIS creates a duration
    std::cout << duration_cast<nanoseconds>(elapsed); // Convert duration to nanoseconds

    When you write :
     duration_cast<nanoseconds>(t1-t0)

    Lets say t1-t2 results in a duration<double,std::milli> ( milliseconds with double precision )

    The template parameters gets resolved to 
    1. _ToDur = std::chrono::nanoseconds
        . This is what you specify : duration_cast<nanoseconds>
        . its equivalent to std::chrono::duration<long long, std::nano>
    2. _Rep = double 
        . The representation type of the input duration 
        . From duration<double, std::milli>
    3. _Period = std::milli
        .The period/ratio of the input duration 
        . From duration<double, std::milli>(represents milliseconds)

std::enable_if<std::chrono::__is_duration<_ToDur>::value, _ToDur>::type

Step by step:

_ToDur = nanoseconds
__is_duration<nanoseconds>::value = true (nanoseconds IS a duration type)
std::enable_if<true, nanoseconds>::type = nanoseconds
So the return type becomes: nanosecond


time_point t0  ──┐
                 ├── SUBTRACTION ──> duration<Rep, Period>
time_point t1  ──┘                           │
                                             ▼
                                    duration_cast<nanoseconds>
                                             │
                                             ▼
                                        nanoseconds

Key Points:
duration_cast doesn't convert time_point to duration
The subtraction t1 - t0 creates the duration
duration_cast converts between different duration types
Template parameters depend on both input and output duration types
The function signature you're looking at is for converting duration → duration, not time_point → duration!






